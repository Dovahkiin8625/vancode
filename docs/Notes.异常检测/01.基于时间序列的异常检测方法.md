---
title: 基于时间序列的异常检测方法
date: 2022-03-01 14:18:24
permalink: /pages/1ac963/
categories:
  - 异常检测
tags:
  - 
---
[TOC]



# 理论：基于时间序列的异常检测方法

**同比，是指在相邻时段中的某一相同时间点进行比较。**

**环比，是指在同一时段中的相邻时间点进行比较。**

## 1 短期环比（SS）

对于时间序列（是指将同一统计指标的数值按其发生的时间先后顺序排列而成的数列）来说，$T$时刻的数值对于$T-1$时刻有很强的依赖性。比如流量在8:00很多，在8:01时刻的概率是很大的，但是如果07:01时刻对于8:01时刻影响不是很大。

首先，我们可以使用最近时间窗口（$T$）内的数据遵循某种趋势的现象来做文章。比如我们将$T$设置为7，则我们取检测值（$now\_value$）和过去7个（记为$i$）点进行比较，如果大于阈值我们将$count$加1，如果$count$超过我们设置的$count\_num$，则认为该点是异常点。
$$
count(\sum_{i=0}^T(now\_value-i)>threshold)>count\_num
$$


上面的公式涉及到$threshold$和$count\_num$两个参数，$count\_num$可以根据的需求进行设置，比如对异常敏感，可以设置$count\_num$小一些，而如果对异常不敏感，可以将$count\_num$设置的大一些。

### 1.1 动态阈值 

通常阈值设置方法会参考过去一段时间内的均值、最大值以及最小值，我们也同样应用此方法。取过去一段时间（比如$T$窗口）的平均值、最大值以及最小值，然后取$max-avg$和$avg−min$的最小值。之所以取最小值的原因是让筛选条件设置的宽松一些，让更多的值通过此条件，减少一些漏报的事件。
$$
threshold=min(max-avg,avg-min)
$$

### 1.2 窗口特征

#### 1.2.1统计特征

##### 3-sigma

一个很直接的异常判定思路是，拿最新3个datapoint的平均值（tail_avg方法）和整个序列比较，看是否偏离总体平均水平太多。怎样算“太多”呢，因为standard deviation表示集合中元素到mean的平均偏移距离，因此最简单就是和它进行比较。在normal distribution（正态分布）中，99.73%的数据都在偏离mean 3个σ (standard deviation 标准差) 的范围内。如果某些datapoint到mean的距离超过这个范围，则认为是异常的。

##### z score

标准分，一个个体到集合mean的偏离，以标准差为单位，表达个体距mean相对“平均偏离水平（std dev表达）”的偏离程度，常用来比对来自不同集合的数据。

在模型中，z_score用来衡量窗口数据中，中间值的偏离程度。

> **算法流程：**
>
> 1. 排除最后一个值
> 2. 求剩余序列的平均值
> 3. 全序列减去上面这个平均值
> 4. 求剩余序列的标准差
> 5. （ 中间三个数的平均值-全序列均值）/ 全序列标准差

##### Grubbs格拉斯测试                          

Grubbs测试是一种从样本中找出outlier的方法，所谓outlier，是指样本中偏离平均值过远的数据，他们有可能是极端情况下的正常数据，也有可能是测量过程中的错误数据。使用Grubbs测试需要总体是正态分布的。

> **算法流程：**
>
> 1. 样本从小到大排序
> 2. 求样本的mean和std.dev
> 3. 计算min/max与mean的差距，更大的那个为可疑值
> 4. 求可疑值的z-score (standard score)，如果大于Grubbs临界值，那么就是outlier；

Grubbs临界值可以查表得到，它由两个值决定：检出水平$\alpha$（越严格越小），样本数量$n$。排除outlier，对剩余序列循环做 1-4 步骤。由于这里需要的是异常判定，只需要判断tail_avg是否outlier即可。

##### moving average（移动平均）

给定一个时间序列和窗口长度$N$，moving average等于当前data point之前$N$个点（包括当前点）的平均值。不停地移动这个窗口，就得到移动平均曲线。

##### cumulative moving average（累加移动平均）

设$\{x_i:i≥1\}$是观察到的数据序列。 累积移动平均线是所有数据的未加权平均值。 如果若干天的值是$x_1,…,x_i$，那么

$$
CMA_i = \frac{x_1+...+x_i}{i}
$$
当有新的值$x_i+1$，那么累积移动平均为

$$
\begin{aligned}
CMA_{i+1}&=\frac{x_1+...+x_i+x_{i+1}}{i+1}\\
&=\frac{x_{i+1}+n\cdot CMA_i}{i+1}\\
&=CMA_i+ \frac{x_{i+1}-CMA_i}{i+1}
\end{aligned}
$$


##### weighted moving average（加权移动平均）

加权移动平均值是先前$w$个数据的加权平均值。 假设$\sum_{j=0}^{w-1}weight_j=1$，其中$weight_j>0$，则加权移动平均值为
$$
WMA_i=\sum_{j=0}^{w-1}weight_j\cdot x_{i-j}
$$
一般

$$
weight_j=\frac{w-j}{w+(w-1)+...+1},for\quad 0\leq j\leq w-1
$$


所以，
$$
WMA_i=\frac{wx_i+(w-1)x_{i-1}+...+2x_{i-w+2}+x_{i-w+1}}{w+(w+1)+...+1}
$$


##### exponential weighted moving average（指数加权移动平均）

指数移动与移动平均有些不同：

1. 并没有时间窗口，用的是从时间序列第一个data point到当前data point之间的所有点。
2. 每个data point的权重不同，离当前时间点越近的点的权重越大，历史时间点的权重随着离当前时间点的距离呈指数衰减，从当前data point往前的data point，权重依次为$\alpha,\alpha(1-\alpha),\alpha(1-\alpha)^2,...,\alpha(1-\alpha)^n$

该算法可以检测一个异常较短时间后发生另外一个异常的情况，异常持续一段时间后可能被判定为正常。

PS：https://www.jianshu.com/p/a2dbd47b3f1a

##### double exponential smoothing（双指数平滑）

假设${Y_t:t≥1}$是观测数据序列，有两个与双指数平滑相关的方程：

$$
S_t=αY_t+(1−α)(S_{t−1}+b_{t−1})\\
b_t=β(S_t−S_{t−1})+(1−β)b_{t−1}
$$
其中$\alpha \in[0,1]$,$\alpha$是数据平滑因子，$\beta\in[0,1]$,$\beta$是趋势平滑因子。

这里，初始值为$S_1=Y_1$，$b_1$有三种可能：

$$
b_1=Y_2-Y_1\\
b_1=\frac{(Y_2-Y_1)+(Y_3-Y_2)+(Y_4-Y_3)}{3}=\frac{Y_4-Y_1}{3}\\
b_1=\frac{Y_n-Y_1}{n-1}\\
$$
预测：

- 单个时刻：$F_{t+1}=S_t+b_t$
- m个时刻：$F_{t+m}=S_t+mb_t$

##### triple exponential smoothing（三指数平滑）（Holt-Winters）

- 乘法模型  multiplicative seasonality

  设${Y_t:t≥1}$是观察到的数据序列，则三指数平滑为

$$
S_t=α\frac{Y_t}{c_{t−L}}+(1−α)(S_{t−1}+b_{t−1}),overall smoothing\\
b_t=β(S_t−S_{t−1})+(1−β)b_{t−1},trend smoothing\\
c_t=γ\frac{Y_t}{S_t}+(1−γ)c_{t−L},seasonalsmoothing\\
$$

  其中$α∈[0,1]$是数据平滑因子，$β∈[0,1]$是趋势平滑因子， $γ∈[0,1]$是季节变化平滑因子。

  预测$m$个时刻：$F_{t+m}=(S_t+mb_t)c_{(t−L+m) mod L}$

  初始值设定：

- 加法模型  additive seasonality

  设${Yt:t≥1}$是观察到的数据序列，则三指数平滑为

  $$
S_t = \alpha(Y_t-c_{t-L})+(1-\alpha)(S_{t-1}+b_{t-1}),overall\quad smoothing\\
  b_t = \beta(S_t-S_{t-1})+(1-\beta)b_{t-1},trend smoothing\\
c_t = \gamma(Y_t-S_{t-1}-b_{t-1})+(1-\gamma)c_{t-L},seasonal smoothing
  $$
  
  
  其中$α∈[0,1]$是数据平滑因子，$β∈[0,1]$是趋势平滑因子， $γ∈[0,1]$是季节变化平滑因子。
  
  预测m个时刻：$F_{t+m}=S_t+mb_t+c_{(t−L+m) mod L}$

##### stddev from average（平均值-标准差）

该算法类似于3sigma准则。当数据服从高斯分布时，数值分布在$(μ−3σ,μ+3σ)$区间内的概率为99.74。所以，可以这么认为，当数据分布区间超过这个区间时，即可认为是异常数据。该算法使用$(t−mean)/std$ 作为特征，用于衡量中间三个值的平均值相对于3σ的距离。

该算法的特点是可以有效屏蔽“在一个点上突变到很大的异常值但在下一个点回落到正常水平”的情况，即屏蔽单点毛刺：因为它使用的是3个点的均值（有效缓和突变），和整个序列比较（均值可能被异常值拉大），导致判断正常。

**算法流程：**

1. 求窗口数据的平均值。 (mean)
2. 求窗口数据的标准差。 (std)
3. 求窗口数据中间3个值的平均值。（t）
4. 使用$(t−mean)/std$作为特征。

##### stddev from moving average（移动平均-标准差）

先求出最后一个点处的指数加权移动平均值，然后再用最新的点和三倍方差方法求异常。

##### stddev from ewma（指数加权移动平均-标准差）

类似于特征3，不过在计算$(t−mean)/std$时，使用的mean，std分别为对窗口数据进行移动加权平均后的平均值以及方差。

##### histogram bins

该算法和以上都不同，它首先将timeseries划分成15个宽度相等的直方，然后判断tail_avg所在直方内的元素是否<=20，如果是，则异常。

直方的个数和元素个数判定需要根据自己的metrics调整，不然在数据量小的时候很容易就异常了。

##### median absolute deviation（中位数绝对偏差）

**median：**大部分情况下我们用mean来表达一个集合的平均水平（average），但是在某些情况下存在少数极大或极小的outlier，拉高或拉低了（skew）整体的mean，造成估计的不准确。此时可以用median（中位数）代替mean描述平均水平。Median的求法很简单，集合排序中间位置即是，如果集合总数为偶数，则取中间二者的平均值。

**median of deviation（MAD）：**同mean一样，对于median我们也需要类似standard deviation这样的指标来表达数据的紧凑/分散程度，即偏离average的平均距离，这就是MAD。MAD顾名思义，是deviation的median，而此时的deviation = abs( 个体 – median )，避免了少量outlier对结果的影响，更robust。

绝对中位差实际求法是用原数据减去中位数后得到的新数据的绝对值的中位数。

1. 原数据-中位值=新数据
2. 新数据的绝对值的中位数作为特征

##### mean subtraction cumulation（平均值减法累积？）

该特征类似于3-sigma准则。

**算法流程**

1. 排除全序列（暂称为all）最后一个值（last datapoint），求剩余序列（暂称为rest，0..length-2）的mean；
2. rest序列中每个元素减去rest的mean，再求标准差；
3. 求窗口数据中间点到rest mean的距离，即 abs(last datapoint – rest mean)；

##### first hour average（前若干-平均值）

和上述算法基本一致，只是比较对象不是整个序列，而是开始一个小时（其实这种这种思想可以推广，只要是时间序列刚开始的一段时间即可）的以内的数据，求出这段时间的均值和标准差和尾部数据（新产生的数据）用三倍方差的方法比较即可。

#### 1.2.2 熵特征

为什么要研究时间序列的熵呢？请看下面两个时间序列：

时间序列（1）：(1,2,1,2,1,2,1,2,1,2,…)

时间序列（2）：(1,1,2,1,2,2,2,2,1,1,…)

在时间序列（1）中，1 和 2 是交替出现的，而在时间序列（2）中，1 和 2 是随机出现的。在这种情况下，时间序列（1）则更加确定，时间序列（2）则更加随机。并且在这种情况下，两个时间序列的统计特征，例如均值，方差，中位数等等则是几乎一致的，说明用之前的统计特征并不足以精准的区分这两种时间序列。

通常来说，要想描述一种确定性与不确定性，**熵（entropy）**是一种不错的指标。对于离散空间而言，一个系统的熵（entropy）可以这样来表示：

$$
entropy(X)=−K\sum_{i=1}^\infty P\{x=x_i\}ln(P\{x=x_i\}).
$$
$K$是和单位选取相关的任意常数。

如果一个系统的熵（entropy）越大，说明这个系统就越混乱；如果一个系统的熵越小，那么说明这个系统就更加确定。

提到时间序列的熵特征，一般来说有几个经典.的例子，那就是 **binned entropy**，**approximate entropy**，**sample entropy**。下面来一一介绍时间序列中这几个经典的熵。

##### Binned Entropy

从熵的定义出发，可以考虑把时间序列$X_T$的取值进行分桶的操作。例如，可以把$[min(X_T),max(X_T)]$这个区间等分为十个小区间，那么时间序列的取值就会分散在这十个桶中。根据这个等距分桶的情况，就可以计算出这个概率分布的熵（entropy）。i.e. Binned Entropy 就可以定义为：

$$
binned\quad entropy(X)=−\sum _{k=0}^{min(maxbin,len(X))}p_k ln(p_k)⋅1(p_k>0)
$$
其中 $p_k$ 表示时间序列 $X_T$ 的取值落在第 $k$ 个桶的比例（概率），$maxbin$ 表示桶的个数， $len(X_T)=T$  表示时间序列 $X_T$ 的长度。

如果一个时间序列的 Binned Entropy 较大，说明这一段时间序列的取值是较为均匀的分布在 $[min(X_T),max(X_T)]$之间的；如果一个时间序列的 Binned Entropy 较小，说明这一段时间序列的取值是集中在某一段上的。

##### Approximate Entropy

回到本节的问题，如何判断一个时间序列是否具备某种趋势还是随机出现呢？这就需要介绍 Approximate Entropy 的概念了，Approximate Entropy 的思想就是把一维空间的时间序列提升到高维空间中，通过高维空间的向量之间的距离或者相似度的判断，来推导出一维空间的时间序列是否存在某种趋势或者确定性。那么，我们现在可以假设时间序列 $X_N:{x_1,⋯,x_N}$的长度是$N$，同时 Approximate Entropy 函数拥有两个参数， $m$ 与 $r$ ，下面来详细介绍 Approximate Entropy 的算法细节。

1. 固定两个参数，正整数 $m$ 和正数 $r$，正整数 $m$ 是为了把时间序列进行一个片段的提取，正数 $r$ 是表示时间序列距离的某个参数。i.e. 需要构造新的 $m$ 维向量如下：
$$
  X_1(m)=(x_1,⋯,x_m)\in \mathbb{R}^m,\\
  X_i(m)=(x_i,⋯,x_{m+i−1})\in \mathbb{R}^m,\\
  X_{N−m+1}(m)=(x_{N−m+1},⋯,x_N)\in \mathbb{R}^m.
$$

2. 通过新的向量$X_1(m),⋯,X_{N−m+1(m)}$，可以计算出哪些向量与 $X_i$ 较为相似。即，
   $$
   C^m_i(r)=(number\quad of\quad X_j(m)\quad such\quad that\quad d(X_i(m),X_j(m))\leq r)/(N−m+1),
   $$
   在这里，距离$d$选择$L^1$，$L^2$，$L^p$，$L^{\infty}$范数。在这个场景下，距离$d$通常选择为$L^{\infty}$范数。

3. 考虑函数$\Phi ^m(r)=(N−m+1)^{−1}\cdot \sum _{i=1}^{N−m+1}ln(C^m_i(r))$

4. Approximate Entropy 可以定义为：
   $$
   ApEn(m,r)=\Phi ^m(r)−\Phi ^{m+1}(r)
   $$
   

   > **Remark:**
   >
   > 1. 正整数 mm 一般可以取值为 22 或者 33， r>0r>0 会基于具体的时间序列具体调整；
   > 2. 如果某条时间序列具有很多重复的片段（repetitive pattern）或者自相似性（self-similarity pattern），那么它的 Approximate Entropy 就会相对小；反之，如果某条时间序列几乎是随机出现的，那么它的 Approximate Entropy 就会相对较大。

##### Sample Entropy

除了 Approximate Entropy，还有另外一个熵的指标可以衡量时间序列，那就是 Sample Entropy，通过自然对数的计算来表示时间序列是否具备某种自相似性。

按照以上 Approximate Entropy 的定义，可以基于$m$与$r$定义两个指标$A$与$B$，分别是

$$
A=\#\{vector\quad pairs\quad having\quad d(X_i(m+1),X_j(m+1))<r\quad of\quad length\quad m+1 \}\\
B=\#\{vector\quad pairs\quad having\quad d(X_i(m),X_j(m))<r\quad of\quad length\quad m \}
$$
其中，$\#$ 表示集合的元素个数。根据度量 $d$ （无论是$L^1$，$L^2$，$L^\infty$ ）的定义可以知道 $A\leq B$，因此 Sample Entropy 总是非负数，即

$$
SampEn=−ln(A/B)\geq 0
$$
备注：

1. Sample Entropy 总是非负数；
2. Sample Entropy 越小表示该时间序列具有越强的自相似性（self similarity）。
3. 通常来说，在 Sample Entropy 的参数选择中，可以选择$m=2,r=0.2×std$。

#### 1.2.3 分段特征

即使时间序列有一定的自相似性（self-similarity），能否说明这两条时间序列就完全相似呢？其实答案是否定的，例如：两个长度都是 1000 的时间序列，

时间序列（1）：$ [1,2] * 500$

时间序列（2）：$[1,2,3,4,5,6,7,8,9,10] * 100$

其中，时间序列（1）是 1 和 2 循环的，时间序列（2）是 1~10 这样循环的，它们从图像上看完全是不一样的曲线，并且它们的 Approximate Entropy 和 Sample Entropy 都是非常小的。那么问题来了，有没有办法提炼出信息，从而表示它们的不同点呢？答案是肯定的。

首先，我们可以回顾一下 Riemann 积分和 Lebesgue 积分的定义和不同之处。按照下面两幅图所示，Riemann 积分是为了算曲线下面所围成的面积，因此把横轴划分成一个又一个的小区间，按照长方形累加的算法来计算面积。而 Lebesgue 积分的算法恰好相反，它是把纵轴切分成一个又一个的小区间，然后也是按照长方形累加的算法来计算面积。

![img](基于时间序列的异常检测方法.assets/clip1541314641.png)

之前的 Binned Entropy 方案是根据值域来进行切分的，好比 Lebesgue 积分的计算方法。现在我们可以按照 Riemann 积分的计算方法来表示一个时间序列的特征，于是就有学者把时间序列按照横轴切分成很多段，每一段使用某个简单函数（线性函数等）来表示，于是就有了以下的方法：

1. 分段线性逼近（Piecewise Linear Approximation）
2. 分段聚合逼近（Piecewise Aggregate Approximation）
3. 分段常数逼近（Piecewise Constant Approximation）

说到这几种算法，其实最本质的思想就是进行数据降维的工作，用少数的数据来进行原始时间序列的表示（Representation）。用数学化的语言来描述时间序列的数据降维（Data Reduction）就是：把原始的时间序列 $\{x_1,…,x_N\}$ 用 $\{x′_1,…,x′_D\}$来表示，其中$D<N$。那么后者就是原始序列的一种表示（representation）。

##### 分段聚合逼近（Piecewise Aggregate Approximation）—- 类似 Riemann 积分

在这种算法中，分段聚合逼近（Piecewise Aggregate Approximation）是一种非常经典的算法。假设原始的时间序列是 $C={x1,…,x_N}$，定义PAA的序列是：$\overline{C}=\{\overline{x}_1,...,\overline{x}_w\}$

其中，
$$
\overline{x}_i=\frac{w}{N}⋅\sum_{j=\frac{N}{w}(i−1)+1}^{\frac{N}{w}i}x_j$​
$$
在这里$1≤i≤w$用图像来表示那就是

![img](基于时间序列的异常检测方法.assets/clip1541314841.png)

至于分段线性逼近（Piecewise Linear Approximation）和分段常数逼近（Piecewise Constant Approximation），只需要在$\overline{x}_i$的定义上稍作修改即可。

##### 符号逼近（Symbolic Approximation）—- 类似 Riemann 积分

在推荐系统的特征工程里面，特征通常来说可以做归一化，二值化，离散化等操作。例如，用户的年龄特征，一般不会直接使用具体的年月日，而是划分为某个区间段，例如 0\~6（婴幼儿时期），7\~12（小学），13\~17（中学），18\~22（大学）等阶段。

其实在得到分段特征之后，分段特征在某种程度上来说依旧是某些连续值，能否把连续值划分为一些离散的值呢？于是就有学者使用一些符号来表示时间序列的关键特征，也就是所谓的符号表示法（Symbolic Representation）。下面来介绍经典的 SAX Representation。

如果我们希望使用 $\alpha$ 个符号，例如 ${l_1,…,l_\alpha}$来表示时间序列。同时考虑分布$N(0,1)$，用

$\{z_{1/\alpha},⋯,z_{(\alpha−1)/\alpha}\}$来表示 Gauss 曲线下方的一些点，而这些点把 Gauss 曲线下方的面积等分成了 $\alpha$段。

算法流程：

1. 正规化（normalization）：也就是该时间序列被映射到均值为零，方差为一的区间内。

2. 分段表示（PAA）：${x_1,⋯,x_N}⇒{\overline{x}_1,⋯,\overline{x}_w}$

3. 符号表示（SAX）：

   * 如果$\overline{x}_i<z_{1/α}$，那么$\hat{X}_i=l_1$；

   * 如果$z_{(j−1)/\alpha}≤\overline{x}_i<z_{j/α}$，那么$\hat{X}_i=l_j$；

   * 如果$\hat{x}_i≥z_{(\alpha−1)/\alpha}$，那么$\hat{X}_i=l_\alpha$。

于是，我们就可以用${l1,⋯,lα}$这$\alpha$个字母来表示原始的时间序列了。

![img](基于时间序列的异常检测方法.assets/clip1541315241.png)

#### 1.2.4 总特征

总的训练特征为：时间窗口特征 + OneHot特征 + Timestamp所属的星期作为特征，label选取时间窗口中间点的标签。因为选择了窗口数据的中间位置的点作为label值，所以在窗口移动过程中，在数据起始点和终结点会丢失 datasize-(windowsize/2) 个数据。（datasize为数据的总量，windowsize为时间窗口的大小)在最终提交的结果中，缺失的数据点的预测结果，用0补充。

### 1.3 分类方法

- 随机森林
- XGBoost
- DNN

### 1.4 聚类方法

#### 1.4.1 k-means聚类

k-均值聚类算法是一种发现数据簇的简单有效的方法。算法步骤如下：

1. 询问用户应当将数据集划分为多少个簇。

2. 随机分配k个记录成为初始簇中心位置。

3. 为每一个记录找到最近的簇中心，因此，从某种意义上来说，每个簇中心“拥有”一个记录的子集，从而表示一个数据集的划分。因此我们有k个簇$C_1,C_2,...,C_k$。

4. 对于k个簇中的每一个簇，找到簇质心，并将簇质心以新的簇中心位置更新。

5. 重复步骤3~5，直到收敛或终止。

> * 虽然其他标准也可以使用，但步骤3中的“最近的”标准通常是[欧氏距离](#1 欧氏距离)。
> * 步骤四中的簇中心的获取方法如下:假设我们有$n$个数据点$(a_1,b_1,c_1),(a_2,b_2,c_2),...,(a_n,b_n,c_n)$，这些点的质心就是这些点的重心，位于点$(\sum a_i/n,\sum b_i/n,\sum c_i/n)$,

当质心不再改变时，该算法终止。换句话说，对所有簇$C_1,C_2,...,C_k$,所有记录分别处于不同的簇且每个簇的中心不再发生变化时，程序终止。另外，当某些收敛准则得到满足时，该算法可能会终止，如**均方根误差(MSE)**无明显变化时。

##### 聚类指标

  $p\in C_i$表示簇i中的每个数据点，$m_j$代表簇i的质心(簇中心)，N是样本总数，k是簇的数量。

* **SSB** 簇之间的平方和
  $$
  SSB=\sum_{i=1}^k n_i\cdot d(m_i,M)^2
  $$

* **MSB** 簇之间的均方
  $$
  MSB=\frac{MSB}{k-1}
  $$

* **SSE** 误差平方和
  $$
  SSE=\sum_{i=1}^k\sum_{p\in C_i}d(p,m_i)^2
  $$

* **MSE** 均方误差
  $$
  MSE=\frac{SSE}{N-k}
  $$

* **伪-F统计量**
  $$
  F_{k-1,N-k}=\frac{MSB}{MSE}=\frac{SSB/k-1}{SSE/N-k}
  $$

##### 簇数确定方法

**1 Elbow方法**

​	手肘法的核心指标是SSE(sum of the squared errors，误差平方和)。随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。并且，当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的真实聚类数。当然，这也是该方法被称为手肘法的原因。

**2 轮廓系数法**



## 2 长期环比（LS）

上面短期环比参考的是短期内的数据，而仅仅有短期内的数据是不够的，我们还需要参考更长时间内数据的总体走势。

通常使用一条曲线对该趋势进行拟合来反应曲线的走势，如果新的数据打破了这种趋势，使曲线变得不平滑，则该点就出现了异常。曲线拟合的方法有很多，比如回归、moving average……。在这里，我们使用EWMA，即指数权重移动平均方法来拟合曲线。在EWMA中，下一点的平均值是由上一点的平均值，加上当前点的实际值修正而来。对于每一个EWMA值，每个数据的权重是不一样的，最近的数据将拥有越高的权重。

有了平均值之后，我们就可以使用3-sigma理论来判断新的input是否超过了容忍范围。比较实际的值是否超出了这个范围就可以知道是否可以告警了。

[![img](基于时间序列的异常检测方法.assets/clip1541172227.png)](http://image.rexking6.top/img/基于时间序列的异常检测方法.assets/clip1541172227.png)

**自己理解：长期环比除了通过拟合、EWMA等方法，ARIMA和RNN也应该能适用**

## 3 同比(chain)

很多监控项都具有一定的周期性，其中以一天为周期的情况比较常见，比如lvs流量在早上4点最低，而在晚上11点最高。为了将监控项的周期性考虑进去，我们选取了某个监控项过去14天的数据。对于某个时刻，将得到14个点可以作为参考值，我们记为$x_i$，其中$i=1,…,14$。

我们先考虑静态阈值的方法来判断input是否异常（突增和突减）。如果input比过去14天同一时刻的最小值乘以一个阈值还小，就会认为该输入为异常点（突减）；而如果input比过去14天同一时刻的最大值乘以一个阈值还大，就会认为该输入为异常点（突增）。

[![img](基于时间序列的异常检测方法.assets/clip1541172603.png)](http://image.rexking6.top/img/基于时间序列的异常检测方法.assets/clip1541172603.png)

静态阈值的方法是根据历史经验得出的值，实际中如何给max threshold和min threshold 是一个需要讨论的话题。根据目前动态阈值的经验规则来说，取平均值是一个比较好的思路。

**自己的理解：同比的方法只能针对周期性的曲线使用，但是同比的方法个人认为只能用来辅助预测，更多的应该是用来检测异常。**

## 4 同比振幅(CA)

同比的方法遇到有些情况就不能检测出异常。比如今天是11.18日，过去14天的历史曲线必然会比今天的曲线低很多。那么今天出了一个小故障，曲线下跌了，相对于过去14天的曲线仍然是高很多的。这样的故障使用方法二就检测不出来，那么我们将如何改进我们的方法呢？一个直觉的说法是，两个曲线虽然不一样高，但是“长得差不多”。那么怎么利用这种“长得差不多”呢？那就是振幅了。

怎么计算$t$时刻的振幅呢？ 我们使用$\frac{x(t)–x(t−1)}{x(t−1)}$来表示振幅。举个例子，例如$t$时刻的流量为900bit，$t−1$时刻的是1000bit，那么可以计算出掉线人数是10%。如果参考过去14天的数据，我们会得到14个振幅值。使用14个振幅的绝对值作为标准，如果$m$时刻的振幅$\frac{m(t)–m(t−1)}{m(t−1)}>amplitude∗threshold$并且$m$时刻的振幅大于$0$，则我们认为该时刻发生突增，而如果$m$时刻的振幅大于$amplitude∗threshold$并且$m$时刻的振幅小于0，则认为该时刻发生突减。其中，
$$
amplitude=max[|\frac{x_i(t)−x_i(t−1)}{x_i(t−1)}|],x=1,2,...,14
$$


## 5 算法组合

上面介绍了四种方法，这四种方法里面，SS和LS是针对非周期性数据的验证方法，而chain和CA是针对周期性数据的验证方法。那这四种方法应该如何选择和使用呢？下面我们介绍两种使用方法：

一、根据周期性的不同来选择合适的方法。这种方法需要首先验证序列是否具有周期性，如果具有周期性则进入左边分支的检测方法，如果没有周期性，则选择进入右分支的检测方法。

[![img](/基于时间序列的异常检测方法.assets/clip1541239974.png)](http://image.rexking6.top/img/基于时间序列的异常检测方法.assets/clip1541239974.png)

上面涉及到了如何检测数据周期的问题，可以使用差分的方法来检测数据是否具有周期性。比如取最近两天的数据来做差分，如果是周期数据，差分后就可以消除波动，然后结合方差阈值判断的判断方法来确定数据的周期性。当然，如果数据波动范围比较大，可以在差分之前先对数据进行归一化（比如z-score）。

二、不区分周期性，直接根据“少数服从多数”的方法来去检测，这种方法比较好理解，在此就不说明了。

[![img](基于时间序列的异常检测方法.assets/clip1541239998.png)](http://image.rexking6.top/img/基于时间序列的异常检测方法.assets/clip1541239998.png)

## 6 其他方法

### Smoothed z-score algorithm

[![img](基于时间序列的异常检测方法.assets/clip1541169228.png)](http://image.rexking6.top/img/基于时间序列的异常检测方法.assets/clip1541169228.png)

主要思想：

1. 利用过去一段历史窗口针对下个节点值做预测（利用平均值，方差信息），若是其超过了一定的阈值，则是个异常点。
2. 对异常点的数值进行平滑，以便评估下下个点是否为异常点。因为不做平滑，由于当前是个异常点，对平均值、方差影响较大，若是下一个点仍是异常点，可能不会识别。

https://zhuanlan.zhihu.com/p/39453139

## 7 相空间重构

- [时间序列模型之相空间重构模型](https://zhuanlan.zhihu.com/p/32910931)

#  机器学习实践：基于机器学习算法的时间序列价格异常检测

异常检测是指检测数据集里面与其他数据不相符的数据点。

异常检测也称为异常值检测，是一种数据挖掘过程，用于确定数据集中发现的异常类型并确定其出现的详细信息。 **在当今世界，由于大量数据无法手动标记异常值，自动异常检测显得至关重要**。 自动异常检测具有广泛的应用，例如欺诈检测，系统健康监测，故障检测以及传感器网络中的事件检测系统等。

你是否有过这样的经历，比如，你经常前往某个目的地进行商务旅行，并且你总是住在同一家酒店。虽然大部分时间那里的房价几乎总是相似的，但偶尔相同的酒店，相同的房间类型，费率却高得令人无法接受，以致于你必须换到另一家酒店，因为你的旅行补贴不能包含这么高的价格。我经历了好几次这样的事情，这让我想到，如果我们能够创建一个模型来自动检测这种价格异常会怎么样呢？

当然某些情况下，一些异常在我们这一生中也只会发生一次，并且我们会事先知道它们的发生，还知道在未来每年的相同时间几乎不会再发生，例如2019年2月2日至2月4日亚特兰大荒谬的酒店价格（译者注：2019年2月3日，第53届超级碗比赛在亚特兰大梅赛德斯——奔驰体育场举行）。

在这篇文章中，我们将探讨不同的异常检测技术，我们的目标是在无监督学习的情况下考察酒店房间价格的时间序列中所在的异常。让我们开始吧！

## **数据获取**

事实上要获取全部数据非常困难，我只能得到一些不完美的数据。

```python
import pandas as pd
import numpy as np
import matplotlib.dates as md
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import host_subplot
import mpl_toolkits.axisartist as AA
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.covariance import EllipticEnvelope
from pyemma import msm
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from mpl_toolkits.mplot3d import Axes3D
from pyemma import msm
%matplotlib inline
```

我们将使用的数据是Personalize Expedia Hotel Searches数据集的子集，读者可在此处找到（*https://www.kaggle.com/c/expedia-personalized-sort/data*，其中训练集为train.csv，测试集为test.csv）。

我们打算按如下方式对训练集train.csv的一个子集进行剪切：

- 选择包含数据点最多的一个酒店property_id = 104517。

- 选择visitor_location_country_id = 219 ，国家ID 219是指美国。 我们这样做是为了统一price_usd列。由于不同国家在显示税费方面有不同的惯例，所以此列的价格可能是每晚或整个住宿的。而我们知道此列向美国游客展示的价格总是每晚不含税的。

- 选择search_room_count = 1。

- 选择我们需要的特征：date_time，price_usd，srch_booking_window，srch_saturday_night_bool。

```python
expedia = pd.read_csv('expedia_train.csv')
df = expedia.loc[expedia['prop_id'] == 104517]
df = df.loc[df['srch_room_count'] == 1]
df = df.loc[df['visitor_location_country_id'] == 219]
df = df[['date_time', 'price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]
```

进行数据剪切后，我们将要适用的数据如下：

```python
df.info()
```

![image-20201023165336899](基于时间序列的异常检测方法.assets/image-20201023165336899.png)

 

```python
df['price_usd'].describe()
```


至此，我们已经检测到一个极端异常，即最大price_usd是5584美元。

如果某单一数据点可被视为相应于其余数据的异常，我们则称之为**Point Anomalies**（例如，购买具有大的交易价值的物品）。我们可以回去检查搜索日志，看看它是什么。 经过一番调查后，我猜它要么是一个错误要么是用户偶然搜索了一个总统套房而无意预订或查看。为了找到更多不是极端的异常，决定删除这个点。



```python
expedia.loc[(expedia['price_usd'] == 5584) &
(expedia['visitor_location_country_id'] == 219)]
```

```python
df = df.loc[df['price_usd'] < 5584]
```



至此，我相信你已经发现我们遗漏了一些东西，也就是说，我们不知道用户搜索的房间类型，标准间的价格可能与大床海景房的价格有很大差异。请记住这一点，但为了示范目的，我们不得不继续。

## **时间序列可视化**





```python
df.plot(x='date_time', y='price_usd', figsize=(12,6))
plt.xlabel('Date time')
plt.ylabel('Price in USD')
plt.title('Time Series of room price by date time of search');
```

![img](基于时间序列的异常检测方法.assets/image-20201023171711751.png)



```python
a = df.loc[df['srch_saturday_night_bool'] == 0, 'price_usd']
b = df.loc[df['srch_saturday_night_bool'] == 1, 'price_usd']
plt.figure(figsize=(10, 6))
plt.hist(a, bins = 50, alpha=0.5, label='Search Non-Sat Night')
plt.hist(b, bins = 50, alpha=0.5, label='Search Sat Night')
plt.legend(loc='upper right')
plt.xlabel('Price')
plt.ylabel('Count')
plt.show()
```

![image-20201023172411751](基于时间序列的异常检测方法.assets/image-20201023172411751.png)一般来说，搜索非周六晚上的价格会更稳定且更低，而周六晚上的价格通常会上涨，看来这家酒店在周末很受欢迎。

## **基于聚类算法的异常检测**

### **k-means 算法**

[k-means](#1.4.1 k-means聚类)是一种应用广泛的聚类算法。它创建了k个类似的数据点集（即聚类），不属于这些组的数据可能会被标记为异常。

#### **1 簇数量选择--Elbow方法**

在我们开始应用k-means算法之前，先使用elbow方法来确定最佳聚类数。

```python
data = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]
n_cluster = range(1, 20)
kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]
scores = [kmeans[i].score(data) for i in range(len(kmeans))]
fig, ax = plt.subplots(figsize=(10,6))
ax.plot(n_cluster, scores)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()
```

![ElbowMethod](基于时间序列的异常检测方法.assets/ElbowMethod.png)

上图曲线中，我们可以看到，图形在聚类簇数目在10之后趋于平稳，这意味着添加更多的簇并不能解释我们相关变量中的更多方差。

#### **2 3D聚类图**

设置$n\_clusters=10 $,并将k-means的输出数据绘制成3D聚类图。

```python
X = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]
X = X.reset_index(drop=True)
km = KMeans(n_clusters=10)
km.fit(X)
km.predict(X)
labels = km.labels_
#Plotting
fig = plt.figure(1, figsize=(7,7))
ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)
ax.scatter(X.iloc[:,0], X.iloc[:,1], X.iloc[:,2],
         c=labels.astype(np.float), edgecolor="k")
ax.set_xlabel("price_usd")
ax.set_ylabel("srch_booking_window")
ax.set_zlabel("srch_saturday_night_bool")
plt.title("K Means", fontsize=14)
```



![3D_graph_K-means](基于时间序列的异常检测方法.assets/3D_graph_K-means.png)

#### **3  PCA(Principal component analysis，主成分分析)**

使用PCA(Principal component analysis，主成分分析)算法确定保留多少个特征是最合适的。

```python
data = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]
X = data.values
# Standard
# 不仅计算训练数据的均值和方差，还会基于计算出来的均值和方差来转换训练数据，
# 从而把数据转换成标准的正太分布
X_std = StandardScaler().fit_transform(X)




# 计算协方差矩阵的特征向量和特征值
mean_vec = np.mean(X_std, axis=0)
cov_mat = np.cov(X_std.T)
eig_vals, eig_vecs = np.linalg.eig(cov_mat)

# Create a list of (eigenvalue, eigenvector) tuples
eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i])
             for i in range(len(eig_vals))]
eig_pairs.sort(key=lambda x: x[0], reverse=True)

# 计算特征值的解释方差
tot = sum(eig_vals)
# Individual explained variance
var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)]

cum_var_exp = np.cumsum(var_exp)  # Cumulative explained variance
# plotting
# plt.figure(figsize=(10, 5))
# plt.bar(range(len(var_exp)), var_exp, alpha=0.3, align='center', label='individual explained variance', color = 'g')
# plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')
# plt.ylabel('Explained variance ratio')
# plt.xlabel('Principal components')
# plt.legend(loc='best')
# plt.show()

```

![PCA](基于时间序列的异常检测方法.assets/PCA.png)

#### **4 特征选择**

```python
# Take useful feature and standardize them
data = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]
X_std = StandardScaler().fit_transform(X)
data = pd.DataFrame(X_std)
# reduce to 2 important features
pca = PCA(n_components=2)
data = pca.fit_transform(data)
# standardize these 2 new features
scaler = StandardScaler()
np_scaled = scaler.fit_transform(data)
data = pd.DataFrame(np_scaled)


```

```python

kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]
df['cluster'] = kmeans[9].predict(data)
df.index = data.index
df['principal_feature1'] = data[0]
df['principal_feature2'] = data[1]
df['cluster'].value_counts()



outliers_fraction = 0.01
data = data.reset_index(drop=True)
# get the distance between each point and its nearest centroid. The biggest distances are considered as anomaly
distance = getDistanceByPoint(data, kmeans[9])
distance = distance.reset_index(drop=True)
number_of_outliers = int(outliers_fraction*len(distance))
threshold = distance.nlargest(number_of_outliers).min()
# anomaly1 contain the anomaly result of the above method Cluster (0:normal, 1:anomaly)


```

#### **5 可视化异常集群视图**

```python
#可视化异常集群视图 visualisation of anomaly with cluster view
fig, ax = plt.subplots(figsize=(10, 6))
colors = {0: 'blue', 1: 'red'}
ax.scatter(df['principal_feature1'], df['principal_feature2'],
           c=df["anomaly1"].apply(lambda x: colors[x]))
plt.xlabel('principal feature1')
plt.ylabel('principal feature2')
plt.show()

```

![anomaly_with_cluster_view](基于时间序列的异常检测方法.assets/cluster_anomaly_view1.png)

#### **6 时间序列可视化**

```python
df = df.sort_values('date_time')
df['date_time_int'] = df.date_time.astype(np.int64)
fig, ax = plt.subplots(figsize=(10,6))
a = df.loc[df['anomaly1'] == 1, ['date_time_int', 'price_usd']] #anomaly
ax.plot(df['date_time_int'], df['price_usd'], color='blue', label='Normal')
ax.scatter(a['date_time_int'],a['price_usd'], color='red', label='Anomaly')
plt.xlabel('Date Time Integer')
plt.ylabel('price in USD')
plt.legend()
plt.show();
```

![time_series_view](基于时间序列的异常检测方法.assets/cluster_ts_view1.png)

#### **7 直方图可视化**

```python
a = df.loc[df['anomaly2'] == 1, 'price_usd']
b = df.loc[df['anomaly2'] == -1, 'price_usd']

fig, axs = plt.subplots(figsize=(10,6))
axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])
plt.show()
```

![cluster_hist_view1](基于时间序列的异常检测方法.assets/cluster_hist_view1.png)

## 基于孤立森林算法的异常检测

#### **1 iforest 算法**

```python
data = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]
scaler = StandardScaler()
np_scaled = scaler.fit_transform(data)
data = pd.DataFrame(np_scaled)
# train isolation forest
model =  IsolationForest(contamination=outliers_fraction)
model.fit(data)

df['anomaly2'] = pd.Series(model.predict(data))
# df['anomaly2'] = df['anomaly2'].map( {1: 0, -1: 1} )

```

#### **2 时序图**

```python
#plotting
fig, ax = plt.subplots(figsize=(10,6))
a = df.loc[df['anomaly2'] == -1, ['date_time_int', 'price_usd']] #anomaly
ax.plot(df['date_time_int'], df['price_usd'], color='blue', label = 'Normal')
ax.scatter(a['date_time_int'],a['price_usd'], color='red', label = 'Anomaly')
plt.legend()
plt.show()

```

![iforest](基于时间序列的异常检测方法.assets/iforest_ts_view.png)

#### **3 直方图**

```python
a = df.loc[df['anomaly2'] == 1, 'price_usd']
b = df.loc[df['anomaly2'] == -1, 'price_usd']
fig, axs = plt.subplots(figsize=(10,6))
axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])
plt.show()
```

![iforest_hits_view](基于时间序列的异常检测方法.assets/iforest_hist_view.png)

## 基于支持向量机算法的异常检测

#### **1 SVM 算法**

```python
data = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]
scaler = StandardScaler()
np_scaled = scaler.fit_transform(data)
data = pd.DataFrame(np_scaled)
# train oneclassSVM 
model = OneClassSVM(nu=outliers_fraction, kernel="rbf", gamma=0.01)
model.fit(data)
 
df['anomaly3'] = pd.Series(model.predict(data))
# df['anomaly3'] = df['anomaly3'].map( {1: 0, -1: 1} )

```

#### **2 时序图**

```python
fig, ax = plt.subplots(figsize=(10,6))
a = df.loc[df['anomaly3'] == -1, ['date_time_int', 'price_usd']] #anomaly
ax.plot(df['date_time_int'], df['price_usd'], color='blue', label ='Normal')
ax.scatter(a['date_time_int'],a['price_usd'], color='red', label = 'Anomaly')
plt.legend()
plt.show();
```

![svm_ts_view](基于时间序列的异常检测方法.assets/svm_ts_view.png)

#### **3 直方图**

```python
a = df.loc[df['anomaly3'] == 1, 'price_usd']
b = df.loc[df['anomaly3'] == -1, 'price_usd']

fig, axs = plt.subplots(figsize=(10,6))
axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])
plt.show();
```

![svm_hits_view](基于时间序列的异常检测方法.assets/svm_hist_view.png)

## 基于高斯分布的异常检测

#### 1 直方图

```python
df_class0 = df.loc[df['srch_saturday_night_bool'] == 0, 'price_usd']
df_class1 = df.loc[df['srch_saturday_night_bool'] == 1, 'price_usd']

fig, axs = plt.subplots(1,2)
df_class0.hist(ax=axs[0], bins=30)
df_class1.hist(ax=axs[1], bins=30);
```

![GD_hits_view](基于时间序列的异常检测方法.assets/GD_hist_view.png)

#### 2 高斯

```python

envelope =  EllipticEnvelope(contamination = outliers_fraction) 
X_train = df_class0.values.reshape(-1,1)
envelope.fit(X_train)
df_class0 = pd.DataFrame(df_class0)
df_class0['deviation'] = envelope.decision_function(X_train)
df_class0['anomaly'] = envelope.predict(X_train)

envelope =  EllipticEnvelope(contamination = outliers_fraction) 
X_train = df_class1.values.reshape(-1,1)
envelope.fit(X_train)
df_class1 = pd.DataFrame(df_class1)
df_class1['deviation'] = envelope.decision_function(X_train)
df_class1['anomaly'] = envelope.predict(X_train)
a0 = df_class0.loc[df_class0['anomaly'] == 1, 'price_usd']
b0 = df_class0.loc[df_class0['anomaly'] == -1, 'price_usd']

a2 = df_class1.loc[df_class1['anomaly'] == 1, 'price_usd']
b2 = df_class1.loc[df_class1['anomaly'] == -1, 'price_usd']

#Gaussian Distribution Anomaly Detection
'''
fig, axs = plt.subplots(1,2)
axs[0].hist([a0,b0], bins=32, stacked=True, color=['blue', 'red'])
axs[1].hist([a2,b2], bins=32, stacked=True, color=['blue', 'red'])
axs[0].set_title("Search Non Saturday Night")
axs[1].set_title("Search Saturday Night")
plt.show()
```



![GD_anomaly_view](基于时间序列的异常检测方法.assets/GD_anomaly_view.png)

## **马尔可夫链的异常检测**





# 论文集

## 针对周期型KPI的异常检测算法

- Time Series Decomposition: Yingying Chen, Ratul Mahajan, Baskar Sridharan, and Zhi-Li Zhang. A provider-side view of web search response time. In Proceedings of the ACM SIGCOMM 2013 confere nce on SIGCOMM, pages 243–254. ACM, 2013.
- Holtwinters: He Yan, Ashley Flavel, Zihui Ge, Alexandre Gerber, Daniel Massey, Christos Papadopoulos, Hiren Shah, and Jennifer Yates. Argus: End-to-end service anomaly detection and localization from an isp’s point of view. In INFOCOM, 2012 Proceedings IEEE, pages 2756–2760. IEEE, 2012.
- AutoEncoder（AE）：[Anomaly Detection异常检测的几种方法](https://www.jianshu.com/p/e31ff328b682)
- Variational AutoEncoder（VAE）：[AIOps探索：基于VAE模型的周期性KPI异常检测方法](https://zhuanlan.zhihu.com/p/45400663)

## 针对稳定型KPI的异常检测算法

- 静态阈值: [Amazon cloudwatch alarm.](http://link.zhihu.com/?target=http%3A//docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/ConsoleAlarms.html)
- Moving Average: David R. Choffnes, Fabián E. Bustamante, and Zihui Ge. Crowdsourcing service-level network event monitoring. In Proceedings of the ACM SIGCOMM 2010 Conf.
- Weighted Moving Average: Balachander Krishnamurthy, Subhabrata Sen, Yin Zhang, and Yan Chen. Sketch-based change detection: methods, evaluation, and applications. In Proceedings of the 3rd ACM
  SIGCOMM conference on Internet measurement, pages 234–247. ACM, 2003.
- Exponentially Weighted Moving Average: Balachander Krishnamurthy, Subhabrata Sen, Yin Zhang, and Yan Chen. Sketch-based change detection: methods, evaluation, and applications. In Proceedings of the 3rd ACM SIGCOMM conference on Internet measurement, pages 234–247. ACM, 2003.
- ARIMA: Yin Zhang, Zihui Ge, Albert Greenberg, and Matthew Roughan. Network anomography. In Proceedings of the 5th ACM SIGCOMM Conference on Internet Measurement, IMC’05, pages 30–30, Berkeley, CA, USA, 2005. USENIX Association.
- [时间序列的自回归模型—从线性代数的角度来看](https://zhuanlan.zhihu.com/p/35093835)

## 针对不稳定型KPI的异常检测算法

- Extreme Value Theory: Siffer A, Fouque P A, Termier A, et al. Anomaly Detection in Streams with Extreme Value Theory[C]//Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017: 1067-1075.
- Wavelet: Paul Barford, Jeffery Kline, David Plonka, and Amos Ron. A signal analysis of network traffic anomalies. In Proceedings of the 2nd ACM SIGCOMM Workshop on Internet measurment, pages 71–82. ACM, 2002.

## 针对异常数据量太少， 采用异常注入算法

- Fernando Silveira, Christophe Diot, Nina Taft, and Ramesh Govindan. Astute: Detecting a different class of traffic anomalies. In Proceedings of the ACM SIGCOMM 2010 Conference, SIGCOMM ’10, pages 267–278. ACM, 2010.
- Anukool Lakhina, Mark Crovella, and Christophe Diot. Mining anomalies using traffic feature distributions. In Proceedings of the 2005 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications, SIGCOMM ’05, pages 217–228. ACM, 2005.
- Anukool Lakhina, Mark Crovella, and Christophe Diot. Diagnosing network-wide traffic anomalies. In Proceedings of the 2004 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications, SIGCOMM ’04, pages 219–230. ACM, 2004

## 针对不同类型的KPI进行聚类

- [Robust and Rapid Clustering of KPIs for Large-Scale Anomaly Detection](https://netman.aiops.org/~peidan/ANM2018/8.DependencyDiscovery/LectureCoverage/2018IWQOS_ROCKA.pdf)

# 工具库

- `sklearn.covariance.EllipticEnvelope`（高斯分布的协方差估计）
- `sklearn.ensemble.IsolationForest`（随机森林）
- `sklearn.neighbors.LocalOutlierFactor`（LOF）

# 裴丹论文解读

## Opprentice

- [智能运维系统（二）](https://zhuanlan.zhihu.com/p/30416563)

## Robust and Rapid Clustering of KPIs for Large-Scale Anomaly Detection

## VAE

- [AIOps探索：基于VAE模型的周期性KPI异常检测方法](https://zhuanlan.zhihu.com/p/45400663)

# KPI比赛分析

## 预处理

- 负样本稀少：负样本过采样
- 缺失值：一阶线性插值填充、均值填充
- 样本权重：增加头部异常样本的权重
- KPI三种形态：周期波动/稳定/不稳定（进行聚类：裴丹18论文）
- 异常值替换：格拉布斯准则剔除替换
- 归一化/正则化：z-score、min-max
- 长异常区间只取前8个时间点

[![img](基于时间序列的异常检测方法.assets/clip1541320628.png)](http://image.rexking6.top/img/基于时间序列的异常检测方法.assets/clip1541320628.png)

## 特征提取

- 统计特征：均值、方差等
- 对比特征：差分、变分等
- 频域特征：频谱分析、小波分析等
- 拟合特征：移动平均、查分平均、权重平均等
- 原始特征
- 深度特征：AR+[LSTM、Seq2Seq]、AttentionLSTM+CNN
- 其他特征：Seasonal Trend Decomposition、PID（P：误差的值、I：误差的积分值、D：误差的微分值）、tsfresh抽取

[![img](基于时间序列的异常检测方法.assets/clip1541320933.png)](http://image.rexking6.top/img/基于时间序列的异常检测方法.assets/clip1541320933.png)

## 模型训练

- DNN+Threshold
- LR+小波分析+随机森林+BiLSTM，加权投票
- xgb

改进：聚类分别训练模型+加入周期特征+加入无监督模型

一分一毛也是心意

赏

[# AIOps](http://blog.rexking6.top/tags/AIOps/)

[《视频分析前沿》二](http://blog.rexking6.top/2018/11/02/《视频分析前沿》二/)



[目标检测-经典网络](http://blog.rexking6.top/2018/11/19/目标检测-经典网络/)

 

# 相关概念

## 1 欧氏距离

### 简介

​	在数学中，欧几里得距离或欧几里得度量是欧几里得空间中两点间“普通”（即直线）距离。使用这个距离，欧氏空间成为度量空间。相关联的范数称为欧几里得范数。较早的文献称之为毕达哥拉斯度量。

### 定义

​	欧几里得度量（euclidean metric）（也称欧氏距离）是一个通常采用的距离定义，指在m维空间中两个点之间的真实距离，或者向量的自然长度（即该点到原点的距离）。在二维和三维空间中的欧氏距离就是两点之间的实际距离。 

### 公式

* **二维空间**：
  $$
  \rho = \sqrt{(x_2-x_1)^2+(y_2-y_1)^2},|X|=\sqrt{x_2^2+y_2^2}.
  $$
  其中，$\rho$ 为点$(x_2,y_2)$与点$(y_1,y_2)$之间的欧式距离，$|x|$为点$(x_2,y_2)$到原点的欧式距离。

* **三维空间：**
  $$
  \rho = \sqrt{(x_2-x_1)^2+(y_2-y_1)^2+(z_2-z_1)^2},|X|=\sqrt{x_2^2+y_2^2+z_2^2}
  $$

* n**维空间：**
  $$
  d(x,y):=\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+...+(x_n-y_n)^2}=\sqrt{\sum_{i=1}^n(x_i-y_i)^2}.
  $$
  

## 2 PCA 主成分分析

主成分分析（Principal component analysis，PCA）是一种常用的无监督学习方法，这一方法利用正交变换把线性相关变量表示的观测数据转换为少数几个由线性无关变量表示的数据 ，线性无关的变量称为主成分。主成分的个数通常下雨原始变量的个数，所以主成分分析属于降维方法。

  

